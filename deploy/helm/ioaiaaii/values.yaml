global:
  imageRegistry: europe-west3-docker.pkg.dev/micro-infra

nameOverride: ""
fullnameOverride: ""
namespaceOverride: ""
commonLabels: {}
commonAnnotations: {}
extraDeploy: []

diagnosticMode:
  enabled: false
  command:
    - sleep
  args:
    - infinity

ingress:
  enabled: true
  pathType: ImplementationSpecific
  hostname: ioaiaaii.net
  ingressClassName: nginx
  path: /
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-cluster-issuer 
    # even if the usage would be extremely low
    nginx.ingress.kubernetes.io/limit-connections: "5"   # Max concurrent connections per client IP
    nginx.ingress.kubernetes.io/limit-rps: "10"          # Max requests per second
    nginx.ingress.kubernetes.io/limit-rate-after: "1000000"  # Allow 1 MB at full speed
    nginx.ingress.kubernetes.io/limit-rate: "100"           # Then throttle to 100 bytes/second
  tls:
  - hosts:
    - ioaiaaii.net
    secretName: letsencrypt-cluster-cert-ioaiaaii
  selfSigned: false
  extraHosts: []
  extraPaths: []
  extraTls: []
  secrets: []
  extraRules: []

networkPolicy:
  enabled: false
  allowExternal: true
  allowExternalEgress: true
  addExternalClientAccess: true
  extraIngress: []
  extraEgress: []
  ingressPodMatchLabels: {}
  ingressNSMatchLabels: {}
  ingressNSPodMatchLabels: {}

telemetry:
  enabled: true
  otelEnvVars:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: "http://otel-collector.observability.svc.cluster.local:4317"
web:
  image:
    repository: micro-repo/ioaiaaii
    tag: null
    pullPolicy: IfNotPresent
    debug: false
  replicaCount: 1
  containerPorts:
    http: 8080

  livenessProbe:
    httpGet:
      path: "/healthz/liveness"
      port: http
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 20
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
    terminationGracePeriodSeconds: 20    

  readinessProbe:
    enabled: true
    httpGet:
      path: "/healthz/readiness"
      port: http
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1

  startupProbe:
    enabled: false

  resourcesPreset: "pico"

  resources: {}

  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001

  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    readOnlyRootFilesystem: true
    privileged: false
    allowPrivilegeEscalation: false
    capabilities:
      # drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  existingConfigmap:

  command: []

  args: []

  automountServiceAccountToken: false
 
  hostAliases: []
 
  deploymentAnnotations: {}
 
  podLabels: {}
 
  podAnnotations: {}

  podAffinityPreset: ""

  podAntiAffinityPreset: soft

  nodeAffinityPreset: {}

  affinity: {}

  nodeSelector: {}

  tolerations: []

  updateStrategy:
    type: RollingUpdate

  podManagementPolicy: OrderedReady

  priorityClassName: ""

  topologySpreadConstraints: []

  pdb:
    create: true
    maxUnavailable: 1

  autoscaling:
    hpa:
      enabled: true
      minReplicas: 2
      maxReplicas: 6
      targetCPU: 70
      targetMemory: 70

  service:
    type: ClusterIP
    ports:
      http: 80
      https: 443

  metrics:
    # -- Enable the export of Prometheus metrics
    enabled: false
    ## Prometheus Operator ServiceMonitor configuration
    serviceMonitor:
      # -- if `true`, creates a Prometheus Operator ServiceMonitor (also requires `metrics.enabled` to be `true`)
      enabled: false
      # --  Namespace in which Prometheus is running
      namespace: ""
      # -- Additional custom annotations for the ServiceMonitor
      annotations: {}
      # -- Extra labels for the ServiceMonitor
      labels: {}
      # -- The name of the label on the target service to use as the job name in Prometheus
      jobLabel: ""
      # --  honorLabels chooses the metric's labels on collisions with target labels
      honorLabels: false
      # -- Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      interval: ""
      # -- Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      scrapeTimeout: ""
      # --  Specify additional relabeling of metrics
      metricRelabelings: []
      # --  Specify general relabeling
      relabelings: []
      # --  Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      selector: {}
